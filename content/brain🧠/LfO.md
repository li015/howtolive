---
Create_time: 2023-03-23 12:58
aliases: 
Unsolved: true
---
[Cite ::  ]
[Last_modified : : `$= dv.current().file.mtime`.]
##### [notes ::  
1. viewpoint difference. [[@liu_imitation_2018]]
2. learn from raw pixel data rather than state-action pairs [[@kroemer_review_2019]]
3. Behavioral Cloning from Observation ([[BCO]])and [[GAIfO]]
4. Another approach in LfO is to design a hand-crafted reward function with expert states and then employ ordinary RL to maximize the episode cumulative reward [9], [12], [25]. However, it is not an easy task to design a proper hand-crafted reward function because there are no mature design paradigms for the reward function and it requires background knowledge from the target application field.
]

```dataview
table 場景, 人, 研究問題, 方法, Cite
from  #LfO
sort 場景, 人, 研究問題, 方法, Cite

```
